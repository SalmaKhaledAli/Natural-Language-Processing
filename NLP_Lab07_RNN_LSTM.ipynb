{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683f8462",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "\n",
    "- Recurrent Neural Network \n",
    "- Long Short-Term Memory \n",
    "- Neural Language Model (Case Study 05)\n",
    "- Conclusion\n",
    "- References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390d286",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "A recurrent neural network (RNN) is a special type of artificial neural network adapted to work for time series data or data that involves sequences. Ordinary feedforward neural networks are only meant for data points that are independent of each other. However, if we have data in a sequence such that one data point depends upon the previous data point, we need to modify the neural network to incorporate the dependencies between these data points. RNNs have the concept of “memory” that helps them store the states or information of previous inputs to generate the next output of the sequence.\n",
    "\n",
    "![RNN Architecture](RNN2.webp)\n",
    "\n",
    "\n",
    "**Advantages and Shortcomings of RNNs**\n",
    "\n",
    "RNNs have various advantages, such as:\n",
    "\n",
    "- Ability to handle sequence data\n",
    "- Ability to handle inputs of varying lengths\n",
    "- Ability to store or “memorize” historical information\n",
    "\n",
    "The disadvantages are:\n",
    "\n",
    "- The computation can be very slow.\n",
    "- The network does not take into account future inputs to make decisions.\n",
    "- Vanishing gradient problem, where the gradients used to compute the weight update may get very close to zero, preventing the network from learning new weights. The deeper the network, the more pronounced this problem is.\n",
    "\n",
    "\n",
    "Well Explained in: https://www.youtube.com/watch?v=AsNTP8Kwu80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07984655",
   "metadata": {},
   "source": [
    "# Long short - Term Memory Network\n",
    "\n",
    "LSTM stands for Long Short-Term Memory, and it is a type of recurrent neural network (RNN) architecture that is designed to overcome the limitations of traditional RNNs in capturing long-term dependencies in sequential data.\n",
    "\n",
    "LSTMs address the issues of RNN by introducing a memory cell that can store information over long periods of time. The memory cell is equipped with gating mechanisms that control the flow of information, allowing the LSTM to selectively retain or forget information based on the current input and the learned patterns.\n",
    "\n",
    "![LSTM Architecture](lstm.png)\n",
    "\n",
    "LSTMs have been widely used in various natural language processing tasks, including language modeling, machine translation, sentiment analysis, and speech recognition, due to their ability to model and understand sequential data with long-term dependencies.\n",
    "\n",
    "Well Explained in: https://www.youtube.com/watch?v=YCzL96nL7j0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db864254",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "Language models are AI models designed to understand and generate human language. They are trained on large amounts of text data and learn the statistical patterns and relationships between words, phrases, and sentences. Language models can be used for a variety of natural language processing tasks, such as text generation, machine translation, sentiment analysis, and question answering.\n",
    "\n",
    "The primary goal of a language model is to predict the next word or sequence of words in a given context. It learns the probability distribution over a vocabulary of words based on the context provided. This is done by utilizing recurrent neural networks (RNNs) or variants like long short-term memory (LSTM) or transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab06530",
   "metadata": {},
   "source": [
    "## Case Study 05\n",
    "\n",
    "In this Case study we are going to implement a Text Generation with LSTM.\n",
    "\n",
    "Text generation is a natural language processing task that involves generating new text based on a given prompt or context. The goal is to generate coherent and contextually relevant text that resembles human-written language.\n",
    "\n",
    "Text generation can be approached in different ways, depending on the specific requirements and techniques used. One common approach is to use language models, such as recurrent neural networks (RNNs), long short-term memory (LSTM) networks, or transformer models.\n",
    "\n",
    "In this lab we will implement the RNN and LSTM approach, the dataset used in this study is https://www.kaggle.com/aashita/nyt-comments ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8a3874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from keras.utils import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "25ebb906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Finding an Expansive View  of a Forgotten People in Niger', 'And Now,  the Dreaded Trump Curse', 'Venezuela’s Descent Into Dictatorship', 'Stain Permeates Basketball Blue Blood', 'Taking Things for Granted', 'The Caged Beast Awakens', 'An Ever-Unfolding Story', 'O’Reilly Thrives as Settlements Add Up', 'Mouse Infestation', 'Divide in G.O.P. Now Threatens Trump Tax Plan']\n"
     ]
    }
   ],
   "source": [
    "def load_headlines(directory):\n",
    "    \"\"\"\n",
    "    Load headlines from CSV files in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Directory path where the headline files are located.\n",
    "\n",
    "    Returns:\n",
    "        list: List of loaded headlines.\n",
    "\n",
    "    \"\"\"\n",
    "    all_headlines = []\n",
    "    \n",
    "    # Iterate over files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if 'Articles' in filename:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            article_df = pd.read_csv(os.path.join(directory, filename))\n",
    "            # Extract the headline values and append to the list\n",
    "            all_headlines.extend(list(article_df.headline.values))\n",
    "            # Break after the first file with 'Articles' in the name\n",
    "            break\n",
    "        \n",
    "    # Filter out headlines with the value \"Unknown\"   \n",
    "    all_headlines = [line for line in all_headlines if line != \"Unknown\"]\n",
    "    return all_headlines\n",
    "\n",
    "\n",
    "directory_path = 'NYC/'\n",
    "headlines = load_headlines(directory_path)\n",
    "print(headlines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8ed4eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d8d6387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \"\"\"\n",
    "    Clean the input data by removing punctuation, converting to lowercase,\n",
    "    and removing non-ASCII characters.\n",
    "\n",
    "    Args:\n",
    "        data (str): Input data to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned data.\n",
    "\n",
    " \n",
    "    \"\"\"\n",
    "    # Remove punctuation characters and convert the word to lower case\n",
    "    data = \"\".join(word for word in data if word not in string.punctuation).lower()\n",
    "    # Encode with UTF-8 and decode with ASCII, ignoring non-ASCII characters\n",
    "    data = data.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9505785d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['finding an expansive view  of a forgotten people in niger', 'and now  the dreaded trump curse', 'venezuelas descent into dictatorship', 'stain permeates basketball blue blood', 'taking things for granted']\n"
     ]
    }
   ],
   "source": [
    "corpus = [clean_data(x) for x in all_headlines]\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7c8800a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_tokens(corpus):\n",
    "    \"\"\"\n",
    "    Convert the corpus into input sequences and calculate the total number of words.\n",
    "\n",
    "    Args:\n",
    "        corpus (list): List of strings representing the corpus.\n",
    "\n",
    "    Returns:\n",
    "        input_sequences (list): List of input sequences.\n",
    "        total_words (int): Total number of words in the vocabulary.\n",
    "\n",
    "    \"\"\"   \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "523a9dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[169, 17], [169, 17, 665], [169, 17, 665, 367], [169, 17, 665, 367, 4], [169, 17, 665, 367, 4, 2], [169, 17, 665, 367, 4, 2, 666], [169, 17, 665, 367, 4, 2, 666, 170], [169, 17, 665, 367, 4, 2, 666, 170, 5], [169, 17, 665, 367, 4, 2, 666, 170, 5, 667]]\n"
     ]
    }
   ],
   "source": [
    "input_sequences, total_words = get_seq_tokens(corpus)\n",
    "print(input_sequences[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f2dd02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_padding(input_sequences):\n",
    "    \"\"\"\n",
    "    Pad the input sequences with zeros and create predictors and labels.\n",
    "\n",
    "    Args:\n",
    "        input_sequences (list): List of input sequences.\n",
    "\n",
    "    Returns:\n",
    "        predictors (numpy.ndarray): Array of input predictors.\n",
    "        label (numpy.ndarray): Array of labels.\n",
    "        max_sequence_len (int): Maximum sequence length.\n",
    "\n",
    "    \"\"\"    \n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "    label = to_categorical(label, num_classes=total_words)\n",
    "\n",
    "    return predictors, label, max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b465a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors, label, max_sequence_len = sequence_padding(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4617aa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 18, 10)            24220     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               44400     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2422)              244622    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 313,242\n",
      "Trainable params: 313,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    # Calculate input length for the Embedding layer\n",
    "    input_len = max_sequence_len - 1\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model and print the model summary\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9021f812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n",
      "Epoch 7/100\n",
      "Epoch 8/100\n",
      "Epoch 9/100\n",
      "Epoch 10/100\n",
      "Epoch 11/100\n",
      "Epoch 12/100\n",
      "Epoch 13/100\n",
      "Epoch 14/100\n",
      "Epoch 15/100\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "Epoch 18/100\n",
      "Epoch 19/100\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "Epoch 24/100\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "Epoch 28/100\n",
      "Epoch 29/100\n",
      "Epoch 30/100\n",
      "Epoch 31/100\n",
      "Epoch 32/100\n",
      "Epoch 33/100\n",
      "Epoch 34/100\n",
      "Epoch 35/100\n",
      "Epoch 36/100\n",
      "Epoch 37/100\n",
      "Epoch 38/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "Epoch 41/100\n",
      "Epoch 42/100\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 45/100\n",
      "Epoch 46/100\n",
      "Epoch 47/100\n",
      "Epoch 48/100\n",
      "Epoch 49/100\n",
      "Epoch 50/100\n",
      "Epoch 51/100\n",
      "Epoch 52/100\n",
      "Epoch 53/100\n",
      "Epoch 54/100\n",
      "Epoch 55/100\n",
      "Epoch 56/100\n",
      "Epoch 57/100\n",
      "Epoch 58/100\n",
      "Epoch 59/100\n",
      "Epoch 60/100\n",
      "Epoch 61/100\n",
      "Epoch 62/100\n",
      "Epoch 63/100\n",
      "Epoch 64/100\n",
      "Epoch 65/100\n",
      "Epoch 66/100\n",
      "Epoch 67/100\n",
      "Epoch 68/100\n",
      "Epoch 69/100\n",
      "Epoch 70/100\n",
      "Epoch 71/100\n",
      "Epoch 72/100\n",
      "Epoch 73/100\n",
      "Epoch 74/100\n",
      "Epoch 75/100\n",
      "Epoch 76/100\n",
      "Epoch 77/100\n",
      "Epoch 78/100\n",
      "Epoch 79/100\n",
      "Epoch 80/100\n",
      "Epoch 81/100\n",
      "Epoch 82/100\n",
      "Epoch 83/100\n",
      "Epoch 84/100\n",
      "Epoch 85/100\n",
      "Epoch 86/100\n",
      "Epoch 87/100\n",
      "Epoch 88/100\n",
      "Epoch 89/100\n",
      "Epoch 90/100\n",
      "Epoch 91/100\n",
      "Epoch 92/100\n",
      "Epoch 93/100\n",
      "Epoch 94/100\n",
      "Epoch 95/100\n",
      "Epoch 96/100\n",
      "Epoch 97/100\n",
      "Epoch 98/100\n",
      "Epoch 99/100\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d1f6928eb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, label, epochs=100, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "825ebd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        # Convert the seed text to token list\n",
    "        token_list = tokenizer.texts_to_sequences([clean_data(seed_text)])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        \n",
    "        # Predict the next word index\n",
    "        predicted = np.argmax(model.predict(token_list),axis=-1)\n",
    "\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "                \n",
    "        # Append the predicted word to the seed text       \n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "83f7cb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Finding An Expansive Zombies Unleashed India\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Science And Technology Am Became Am Am Learn\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Donald Trump Unleashed Civics\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "New York Coal Mayor Coming Dreaded\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"finding an expansive\", 3, model, max_sequence_len))\n",
    "print (generate_text(\"science and technology\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"Donald trump\", 2, model, max_sequence_len))\n",
    "print (generate_text(\"New york\", 4, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a13b8",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "there are several ways to further improve the Language model. Here are some suggestions:\n",
    "\n",
    "- **Adding more data:** Increasing the size of your training dataset can often lead to better performance. If possible, consider acquiring more data or exploring additional sources to expand the training data.\n",
    "\n",
    "- **Fine-tuning the model architecture:** Experiment with different model architectures, such as increasing or decreasing the number of layers, adjusting the number of units in each layer, or trying different types of layers (e.g., LSTM, GRU, or Transformer). You can also consider incorporating techniques like attention mechanisms or residual connections to improve the model's ability to capture long-range dependencies.\n",
    "\n",
    "- **Fine-tuning hyperparameters:** Hyperparameters like the number of epochs, learning rate, batch size, and activation functions can significantly impact model performance. Try different combinations of these hyperparameters to find the optimal configuration. You can also utilize techniques like grid search or random search to systematically explore the hyperparameter space.\n",
    "\n",
    "- **Regularization techniques:** Regularization techniques like dropout and L2 regularization can help prevent overfitting and improve the generalization of the model. Experiment with different dropout rates or regularization strengths to strike a balance between model complexity and generalization.\n",
    "\n",
    "- **Transfer learning and pretraining:** If you have access to a pre-trained language model, such as GPT or BERT, you can leverage transfer learning by fine-tuning the pre-trained model on your specific task or domain. This can help improve the performance of your model, especially if you have limited training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375185f7",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/\n",
    "- https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n",
    "- https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7\n",
    "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- https://towardsdatascience.com/language-modeling-with-lstms-in-pytorch-381a26badcbf\n",
    "- https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275\n",
    "- https://www.analyticsvidhya.com/blog/2022/02/explaining-text-generation-with-lstm/\n",
    "- https://www.kaggle.com/code/shivamb/beginners-guide-to-text-generation-using-lstms/notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
