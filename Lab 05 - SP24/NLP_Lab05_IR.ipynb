{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retreival\n",
    "\n",
    "Information retrieval (IR) is the process of retrieving relevant information from a large collection of data based on a user's query. It encompasses various techniques and methodologies aimed at efficiently and effectively locating information that matches a user's information need. In IR, documents are typically represented in a structured format (such as text documents, web pages, or multimedia files), and retrieval is often performed using keyword-based search or more advanced techniques like natural language processing and machine learning. IR systems are widely used in search engines, digital libraries, document management systems, and recommendation systems to help users find relevant information amidst vast amounts of data.\n",
    "\n",
    "\n",
    "### Information Retrieval Search Model:\n",
    "\n",
    "- Boolean model: In the Boolean model, a query is represented as a Boolean expression of terms, and a document is either relevant or non-relevant to the query. This model is simple and efficient, but it can be too restrictive and may not account for partial matches or term frequency.\n",
    "\n",
    "- Vector space model: In the vector space model, a document is represented as a vector of term weights, and a query is represented as a vector of term weights as well. The similarity between the query vector and each document vector is computed using a similarity measure, such as cosine similarity. This model is flexible and can handle partial matches and term frequency, but it may suffer from the \"curse of dimensionality\" and may require normalization and tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Alt text that describes the graphic](https://blog.langchain.dev/content/images/2023/02/ingest.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the documents using LangChain\n",
    "\n",
    "Langchain uses document loaders to bring in information from various sources and prepare it for processing. These loaders act like data connectors, fetching information and converting it into a format Langchain understands.\n",
    "\n",
    "There are a lot of document loaders in LangChain:\n",
    "\n",
    "- TextLoader\n",
    "- CSVLoader\n",
    "- DirectoryLoader\n",
    "- PyPDFLoader\n",
    "- ArxivLoader\n",
    "- Docx2txtLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Companies_Documents/Cisco'\n",
    "\n",
    "def load_docs(directory):\n",
    "    \n",
    "    loader = DirectoryLoader(directory, glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "documents = load_docs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Chunking (Splitting)\n",
    "\n",
    "Chunking is the process of breaking down large pieces of text into smaller segments. It’s an essential technique that helps optimize the relevance of the content we get back from a vector database once we use the LLM to embed content.\n",
    "\n",
    "- The RecursiveCharacterTextSplitter takes a large text and splits it based on a specified chunk size. It does this by using a set of characters. The default characters provided to it are [\"\\n\\n\", \"\\n\", \" \", \"\"].\n",
    "\n",
    "It takes in the large text then tries to split it by the first character \\n\\n. If the first split by \\n\\n is still large then it moves to the next character which is \\n and tries to split by it. If it is still larger than our specified chunk size it moves to the next character in the set until we get a split that is less than our specified chunk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    }
   ],
   "source": [
    "def split_docs(documents,chunk_size=1000,chunk_overlap=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "docs = split_docs(documents)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings of the documents using sentiment transformers\n",
    "\n",
    "SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can find their documentation here:https://www.sbert.net/\n",
    "\n",
    "You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similarity, semantic search, or paraphrase mining.\n",
    "\n",
    "You can also find all the pre-trained model here: https://www.sbert.net/docs/pretrained_models.html . Some of these models support Arabic and +50 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = SentenceTransformerEmbeddings(model_name='all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vector store database\n",
    "\n",
    "Chroma is a Vector Store / Vector DB by the company Chroma. Chroma DB like many other Vector Stores out there, is for storing and retrieving vector embeddings.\n",
    "\n",
    "At present Chroma does not provide any hosting services. Store the data locally in the local file system when creating applications around Chroma. Though Chroma is planning to build a hosting service in the near future. Chroma DB offers different ways to store vector embeddings. You can store them In-memory, you can save and load them In-memory\n",
    "\n",
    "Other examples on vector stores databases:\n",
    "- FAISS\n",
    "- Qdrant\n",
    "- Weaviate\n",
    "- Deep Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'docs/chroma/'\n",
    "\n",
    "db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={'k':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expectation for the p rohibition  of forced labor.  These nonconformities indicated risks for forced labor or bonded labor. These nonconformities \n",
      "ranged in severity.  \n",
      "The most common  nonconformities we identified related to workers paying small fees pertaining to the recruit ment process, such as small one -\n",
      "time fees for health examinations, deposits, or transportation fees often amounting to less than five percent of the  worker’s  monthly salary. \n",
      "These fees were sometimes reimbursed after commencement of employment. Our teams c ontinue to work with suppliers to develop models in \n",
      "which employers pay healthcare providers directly for health examinations, eliminating the need for workers to be reimbursed.  \n",
      "Less often , we identified  risks of bonded labor, a type of forced labor. Worke rs become bonded by debt when they are forced to work in order to\n",
      "Companies_Documents/Cisco/cisco-modern-slavery-statement.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Does the company prohibit 'recruitment fees' to workers?\"\n",
    "# matching_docs = retriever.get_relevant_documents(query)\n",
    "matching_docs = db.similarity_search(query)\n",
    "\n",
    "print(matching_docs[0].page_content)\n",
    "print(matching_docs[0].metadata['source'])\n",
    "matching_docs[0].metadata['page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For each pay period, workers shall be provided with a timely and understandable wage statement \n",
      "that includes sufficient information to verify accurate compensation for work performed. All use of \n",
      "temporary, dispatch and outsourced labor shall be within the limits of the local law.  \n",
      " \n",
      "5) Non-Discrimination/Non -Harassment/Humane Treatment  \n",
      "Participants shall commit to a workplace free of harassment and unlawful discrimination. There \n",
      "shall be no harsh or inhumane treatment including violence, gender -based violence, sexual  \n",
      "harassment,  sexual  abuse,  corporal  punishment,  mental  or physical  coercion,  bullying, public \n",
      "shaming, or verbal abuse of workers; nor is there to be the threat of any such treatment. Companies shall not engage in discrimination or harassment based on race, color, age, gender, \n",
      "sexual orientation, gender identity or expression,  ethnicity or national origin, disability, pregnancy,\n",
      "Companies_Documents/Cisco/RBACodeofConduct8.0_English.pdf\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "query = \"employer pays\"\n",
    "# matching_docs = db.similarity_search(query)\n",
    "matching_docs = retriever.get_relevant_documents(query)\n",
    "print(matching_docs[0].page_content)\n",
    "print(matching_docs[0].metadata['source'])\n",
    "print(matching_docs[0].metadata['page'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving based on a specific keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def search_keywords_in_folder(folder_path, search_keywords):\n",
    "    results = []\n",
    "    # Iterate through files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):  # Check if file is PDF\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            with open(filepath, 'rb') as pdf_file:\n",
    "                pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "                # Iterate through pages of the PDF\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    page_obj = pdf_reader.pages[page_num]\n",
    "                    text = page_obj.extract_text()\n",
    "                    # Check if the keyword is in the text\n",
    "                    if search_keywords.lower() in text.lower():\n",
    "                        results.append((text, filename, page_num + 1))\n",
    "                        break  # Stop searching this file once keyword is found\n",
    "    if results:\n",
    "        return \"Yes\", results\n",
    "    else:\n",
    "        return \"\", None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search result: \n"
     ]
    }
   ],
   "source": [
    "result, found_locations = search_keywords_in_folder(directory, 'employer pays')\n",
    "print(\"Search result:\", result)\n",
    "if result == \"Yes\":\n",
    "    print(\"Keywords found in the following locations:\")\n",
    "    for content, filename, page_num in found_locations:\n",
    "        print(f\"Content: {content}, File: {filename}, Page: {page_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://medium.com/@varsha.rainer/document-loaders-in-langchain-7c2db9851123\n",
    "- https://docs.langflow.org/components/text-splitters#:~:text=The%20RecursiveCharacterTextSplitter%20splits%20the%20text,size%20exceeds%20a%20specified%20threshold.\n",
    "- https://medium.com/@cronozzz.rocks/splitting-large-documents-text-splitters-langchain-7c7bfa899267\n",
    "- https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore/\n",
    "- https://jorgepit-14189.medium.com/get-started-with-chroma-db-and-retrieval-models-using-langchain-87784ffaa918"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
