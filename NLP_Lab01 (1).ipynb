{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of content:\n",
    "\n",
    "- Part1: Text Preprocessing:\n",
    " - Preprocessing on English text\n",
    " - Preprocessing on Arabic text\n",
    "- Part2: Features Extraction techniques\n",
    " - One-hot Encoding\n",
    " - Bag of words (BOW)\n",
    " - N- grams\n",
    " - Sentiment Analysis Application\n",
    "- Part3: Parsing \n",
    " - Dependency Parsing\n",
    " - Consistuency Parsing\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing is an essential step in natural language processing tasks. It helps to clean and transform raw text data into a format that can be easily processed by machine learning models or other algorithms. \n",
    "\n",
    "Some of the reasons why text preprocessing is important include:\n",
    "\n",
    "- **Noise reduction:** Text data often contains noise such as special characters, punctuation, and stop words that can negatively impact the performance of machine learning models. Text preprocessing techniques can help remove or reduce these noise elements.\n",
    "\n",
    "- **Consistency:** Text preprocessing techniques such as stemming and lemmatization can help reduce the variations of words and bring them to their root form, making it easier for algorithms to understand and process them.\n",
    "\n",
    "- **Efficiency:** Preprocessing can also help reduce the size of the data and the time required for computation by removing irrelevant or redundant information.\n",
    "\n",
    "- **Better accuracy:** Preprocessing can improve the accuracy of models by transforming the data into a more uniform and understandable format, allowing the algorithms to extract meaningful patterns and relationships from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the general steps for preprocessing text:\n",
    "\n",
    "- Lower Casing\n",
    "- Tokenization\n",
    "- Punctuation Mark Removal\n",
    "- Stop Word Removal\n",
    "- Stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing on English text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Library\n",
    "\n",
    "Natural Language Toolkit (NLTK) is a Python library designed for working with human language data. It provides tools for various natural language processing (NLP) tasks such as tokenization, stemming, tagging, parsing, and semantic reasoning. NLTK includes a wide variety of corpora, lexical resources, and pre-trained models for many languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK documentation: https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries for the NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Salma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Salma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Salma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'themselves', 'only', 'down', 'ours', 'its', \"you're\", 'to', 'an', 'couldn', 'over', 'doesn', 'more', 'am', 'having', 'hers', 'wasn', 'will', 'theirs', 'hadn', 'most', \"you've\", 'nor', 'mightn', 'few', \"aren't\", 'can', 'these', 'because', 'some', 'how', 'don', \"couldn't\", 'further', 'very', 'he', 'was', \"weren't\", 'whom', 'ma', \"didn't\", 'off', 'for', 'it', 'them', 'were', 'after', \"haven't\", 'hasn', 'between', 'no', 'too', 'is', 'a', \"wasn't\", 've', 'not', 'other', 'own', 'just', 'had', 'who', 'ain', 'be', 'up', 'as', 'she', 'shouldn', 'they', 'what', 'him', 'weren', 'under', 'did', 'her', 'our', 'have', \"needn't\", \"won't\", 'mustn', 'about', \"you'd\", 'now', 'do', 'needn', \"wouldn't\", 'than', 'll', 'when', 'those', 'their', 'does', 'myself', 'haven', \"she's\", 'herself', 'below', 'but', 'doing', \"don't\", 'into', 'then', 'that', 'by', 'has', 'which', 'out', 'with', 'through', 'why', 'wouldn', 'again', 're', \"it's\", \"hasn't\", 'yourselves', 'been', 'against', 'once', 'should', 'and', 'ourselves', 'didn', 'm', 'same', 'aren', 'won', 't', 'isn', 'himself', \"should've\", 'here', 'while', 'or', 'such', 'shan', 'my', \"that'll\", 'his', 'both', 'of', 'in', 'at', 'your', 'during', 'all', \"shan't\", 'each', 'itself', 'from', 'o', 'until', 'd', 'y', \"shouldn't\", 'me', \"doesn't\", 'you', 'yourself', 'there', 'yours', 'we', 'so', \"isn't\", \"you'll\", 'being', 's', 'the', \"hadn't\", 'on', 'this', 'where', 'before', 'i', 'are', 'any', 'above', 'if', \"mightn't\", \"mustn't\"}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This text is used to demonstrate Text Preprocessing in NLP.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split text into words or phrases (tokens). This is done to make the text data more manageable and easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'text',\n",
       " 'is',\n",
       " 'used',\n",
       " 'to',\n",
       " 'demonstrate',\n",
       " 'Text',\n",
       " 'Preprocessing',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way.\n",
    "\n",
    "This is more helpful for text featurization techniques like frequency, tfidf as it helps to combine the same words together thereby reducing the duplication and get correct counts / tfidf values.\n",
    "\n",
    "This may not be helpful when we do tasks like Part of Speech tagging (where proper casing gives some information about Nouns and so on) and Sentiment Analysis (where upper casing refers to anger and so on).\n",
    "\n",
    "So, applying the lower casing is depeneding on the task in hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'text',\n",
       " 'is',\n",
       " 'used',\n",
       " 'to',\n",
       " 'demonstrate',\n",
       " 'text',\n",
       " 'preprocessing',\n",
       " 'in',\n",
       " 'nlp',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower casing\n",
    "tokens = [token.lower() for token in tokens]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, we will remove all the tokens (words) from the list tokens that are present in the NLTK stopwords list, which contains common words like \"the\", \"a\", \"an\", \"and\", etc. This is done to remove the words that do not carry much meaning and can be safely ignored during text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'used', 'demonstrate', 'text', 'preprocessing', 'nlp', '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop words removal\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, remove punctuation marks from the tokenized list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'used', 'demonstrate', 'text', 'preprocessing', 'nlp']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Punctuation removal\n",
    "tokens = [token for token in tokens if token not in string.punctuation]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove suffixes from words to obtain their root form, we use stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'use', 'demonstr', 'text', 'preprocess', 'nlp']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to convert words to their base or dictionary form (called \"lemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'used', 'demonstrate', 'text', 'preprocessing', 'nlp']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatized_tokens = [lem.lemmatize(token) for token in tokens]\n",
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that: lemmatization takes into account the context and part-of-speech (POS) of the word, while stemming doesn't. This means that lemmatization can sometimes produce better results than stemming, especially in cases where words have multiple meanings or forms. However, stemming is generally faster and more efficient than lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', 'NN'),\n",
       " ('used', 'VBN'),\n",
       " ('demonstrate', 'NN'),\n",
       " ('text', 'NN'),\n",
       " ('preprocessing', 'NN'),\n",
       " ('nlp', 'NN')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part of speech tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing on Arabic text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many tools now for preprocessing Arabic text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyArabic Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Library documentation: https://pypi.org/project/PyArabic/\n",
    "- Library features: https://github.com/linuxscout/pyarabic/blob/master/doc/features.md "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Features:\n",
    " - Arabic letters classification\n",
    " - Text tokenization into words or sentences\n",
    " - Strip Harakat ( all, except Shadda, tatweel, last_haraka)\n",
    " - Sperate and join Letters and Harakat\n",
    " - Reduce tashkeel\n",
    " - Mesure tashkeel similarity ( Harakats, fully or partially vocalized, similarity with a template)\n",
    " - Letters normalization ( Ligatures and Hamza)\n",
    " - Numbers to words\n",
    " - Extract numerical phrases\n",
    " - Pre-vocalization of numerical phrases\n",
    " - Unshiping texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyArabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "from pyarabic.araby import strip_tashkeel, strip_tatweel\n",
    "from pyarabic.araby import tokenize as araby_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"اَللُّغَة اَلْعَرَبِيَّةِ وَاحِدَةً مِنْ أَصْعَبِ اَللُّغَاتِ اَلسَّامِيَّةِ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['اَللُّغَة',\n",
       " 'اَلْعَرَبِيَّةِ',\n",
       " 'وَاحِدَةً',\n",
       " 'مِنْ',\n",
       " 'أَصْعَبِ',\n",
       " 'اَللُّغَاتِ',\n",
       " 'اَلسَّامِيَّةِ']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize text\n",
    "tokens = araby_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['اللغة', 'العربية', 'واحدة', 'من', 'أصعب', 'اللغات', 'السامية']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove diacritics (التشكيل)\n",
    "without_tashkeel = []\n",
    "for token in tokens:\n",
    "    \n",
    "    without_tashkeel.append(strip_tashkeel(token))\n",
    "without_tashkeel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'العربية'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove tatweel\n",
    "text_without_tatweel = strip_tatweel(\"العـــــربية\")\n",
    "text_without_tatweel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get tweets and these tweets contain another languages, you could remove tashkeel and filter out non-Arabic words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['اسم', 'الكلب', 'في', 'اللغة', 'الإنجليزية', 'واسم', 'الحمار']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyarabic.araby import tokenize, is_arabicrange, strip_tashkeel\n",
    "text = u\"ِاسمٌ الكلبِ في اللغةِ الإنجليزية Dog واسمُ الحمارِ Donky\"\n",
    "tokenize(text, conditions=is_arabicrange, morphs=strip_tashkeel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Tashaphyne\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/00/e7a413e76c2a9ef2361809f27df6f355fc848c71dbbea78e3633e1d46523/Tashaphyne-0.3.6-py3-none-any.whl (251kB)\n",
      "Requirement already satisfied: pyarabic in c:\\users\\salma\\anaconda3\\lib\\site-packages (from Tashaphyne) (0.6.15)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\salma\\anaconda3\\lib\\site-packages (from pyarabic->Tashaphyne) (1.16.0)\n",
      "Installing collected packages: Tashaphyne\n",
      "Successfully installed Tashaphyne-0.3.6\n"
     ]
    }
   ],
   "source": [
    "#!pip install Tashaphyne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import pyarabic.arabrepr\n",
    "from tashaphyne.stemming import ArabicLightStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا',\n",
       " 'أبٌ',\n",
       " 'أخٌ',\n",
       " 'حمٌ',\n",
       " 'فو',\n",
       " 'أنتِ',\n",
       " 'يناير',\n",
       " 'فبراير',\n",
       " 'مارس',\n",
       " 'أبريل',\n",
       " 'مايو',\n",
       " 'يونيو',\n",
       " 'يوليو',\n",
       " 'أغسطس',\n",
       " 'سبتمبر',\n",
       " 'أكتوبر',\n",
       " 'نوفمبر',\n",
       " 'ديسمبر',\n",
       " 'جانفي',\n",
       " 'فيفري',\n",
       " 'مارس',\n",
       " 'أفريل',\n",
       " 'ماي',\n",
       " 'جوان',\n",
       " 'جويلية',\n",
       " 'أوت',\n",
       " 'كانون',\n",
       " 'شباط',\n",
       " 'آذار',\n",
       " 'نيسان',\n",
       " 'أيار',\n",
       " 'حزيران',\n",
       " 'تموز',\n",
       " 'آب',\n",
       " 'أيلول',\n",
       " 'تشرين',\n",
       " 'دولار',\n",
       " 'دينار',\n",
       " 'ريال',\n",
       " 'درهم',\n",
       " 'ليرة',\n",
       " 'جنيه',\n",
       " 'قرش',\n",
       " 'مليم',\n",
       " 'فلس',\n",
       " 'هللة',\n",
       " 'سنتيم',\n",
       " 'يورو',\n",
       " 'ين',\n",
       " 'يوان',\n",
       " 'شيكل',\n",
       " 'واحد',\n",
       " 'اثنان',\n",
       " 'ثلاثة',\n",
       " 'أربعة',\n",
       " 'خمسة',\n",
       " 'ستة',\n",
       " 'سبعة',\n",
       " 'ثمانية',\n",
       " 'تسعة',\n",
       " 'عشرة',\n",
       " 'أحد',\n",
       " 'اثنا',\n",
       " 'اثني',\n",
       " 'إحدى',\n",
       " 'ثلاث',\n",
       " 'أربع',\n",
       " 'خمس',\n",
       " 'ست',\n",
       " 'سبع',\n",
       " 'ثماني',\n",
       " 'تسع',\n",
       " 'عشر',\n",
       " 'ثمان',\n",
       " 'سبت',\n",
       " 'أحد',\n",
       " 'اثنين',\n",
       " 'ثلاثاء',\n",
       " 'أربعاء',\n",
       " 'خميس',\n",
       " 'جمعة',\n",
       " 'أول',\n",
       " 'ثان',\n",
       " 'ثاني',\n",
       " 'ثالث',\n",
       " 'رابع',\n",
       " 'خامس',\n",
       " 'سادس',\n",
       " 'سابع',\n",
       " 'ثامن',\n",
       " 'تاسع',\n",
       " 'عاشر',\n",
       " 'حادي',\n",
       " 'أ',\n",
       " 'ب',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ك',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ء',\n",
       " 'ى',\n",
       " 'آ',\n",
       " 'ؤ',\n",
       " 'ئ',\n",
       " 'أ',\n",
       " 'ة',\n",
       " 'ألف',\n",
       " 'باء',\n",
       " 'تاء',\n",
       " 'ثاء',\n",
       " 'جيم',\n",
       " 'حاء',\n",
       " 'خاء',\n",
       " 'دال',\n",
       " 'ذال',\n",
       " 'راء',\n",
       " 'زاي',\n",
       " 'سين',\n",
       " 'شين',\n",
       " 'صاد',\n",
       " 'ضاد',\n",
       " 'طاء',\n",
       " 'ظاء',\n",
       " 'عين',\n",
       " 'غين',\n",
       " 'فاء',\n",
       " 'قاف',\n",
       " 'كاف',\n",
       " 'لام',\n",
       " 'ميم',\n",
       " 'نون',\n",
       " 'هاء',\n",
       " 'واو',\n",
       " 'ياء',\n",
       " 'همزة',\n",
       " 'ي',\n",
       " 'نا',\n",
       " 'ك',\n",
       " 'كن',\n",
       " 'ه',\n",
       " 'إياه',\n",
       " 'إياها',\n",
       " 'إياهما',\n",
       " 'إياهم',\n",
       " 'إياهن',\n",
       " 'إياك',\n",
       " 'إياكما',\n",
       " 'إياكم',\n",
       " 'إياك',\n",
       " 'إياكن',\n",
       " 'إياي',\n",
       " 'إيانا',\n",
       " 'أولالك',\n",
       " 'تانِ',\n",
       " 'تانِك',\n",
       " 'تِه',\n",
       " 'تِي',\n",
       " 'تَيْنِ',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'ذانِ',\n",
       " 'ذِه',\n",
       " 'ذِي',\n",
       " 'ذَيْنِ',\n",
       " 'هَؤلاء',\n",
       " 'هَاتانِ',\n",
       " 'هَاتِه',\n",
       " 'هَاتِي',\n",
       " 'هَاتَيْنِ',\n",
       " 'هَذا',\n",
       " 'هَذانِ',\n",
       " 'هَذِه',\n",
       " 'هَذِي',\n",
       " 'هَذَيْنِ',\n",
       " 'الألى',\n",
       " 'الألاء',\n",
       " 'أل',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'ذيت',\n",
       " 'كأيّ',\n",
       " 'كأيّن',\n",
       " 'بضع',\n",
       " 'فلان',\n",
       " 'وا',\n",
       " 'آمينَ',\n",
       " 'آهِ',\n",
       " 'آهٍ',\n",
       " 'آهاً',\n",
       " 'أُفٍّ',\n",
       " 'أُفٍّ',\n",
       " 'أفٍّ',\n",
       " 'أمامك',\n",
       " 'أمامكَ',\n",
       " 'أوّهْ',\n",
       " 'إلَيْكَ',\n",
       " 'إلَيْكَ',\n",
       " 'إليكَ',\n",
       " 'إليكنّ',\n",
       " 'إيهٍ',\n",
       " 'بخٍ',\n",
       " 'بسّ',\n",
       " 'بَسْ',\n",
       " 'بطآن',\n",
       " 'بَلْهَ',\n",
       " 'حاي',\n",
       " 'حَذارِ',\n",
       " 'حيَّ',\n",
       " 'حيَّ',\n",
       " 'دونك',\n",
       " 'رويدك',\n",
       " 'سرعان',\n",
       " 'شتانَ',\n",
       " 'شَتَّانَ',\n",
       " 'صهْ',\n",
       " 'صهٍ',\n",
       " 'طاق',\n",
       " 'طَق',\n",
       " 'عَدَسْ',\n",
       " 'كِخ',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانكم',\n",
       " 'مكانكما',\n",
       " 'مكانكنّ',\n",
       " 'نَخْ',\n",
       " 'هاكَ',\n",
       " 'هَجْ',\n",
       " 'هلم',\n",
       " 'هيّا',\n",
       " 'هَيْهات',\n",
       " 'وا',\n",
       " 'واهاً',\n",
       " 'وراءَك',\n",
       " 'وُشْكَانَ',\n",
       " 'وَيْ',\n",
       " 'يفعلان',\n",
       " 'تفعلان',\n",
       " 'يفعلون',\n",
       " 'تفعلون',\n",
       " 'تفعلين',\n",
       " 'اتخذ',\n",
       " 'ألفى',\n",
       " 'تخذ',\n",
       " 'ترك',\n",
       " 'تعلَّم',\n",
       " 'جعل',\n",
       " 'حجا',\n",
       " 'حبيب',\n",
       " 'خال',\n",
       " 'حسب',\n",
       " 'خال',\n",
       " 'درى',\n",
       " 'رأى',\n",
       " 'زعم',\n",
       " 'صبر',\n",
       " 'ظنَّ',\n",
       " 'عدَّ',\n",
       " 'علم',\n",
       " 'غادر',\n",
       " 'ذهب',\n",
       " 'وجد',\n",
       " 'ورد',\n",
       " 'وهب',\n",
       " 'أسكن',\n",
       " 'أطعم',\n",
       " 'أعطى',\n",
       " 'رزق',\n",
       " 'زود',\n",
       " 'سقى',\n",
       " 'كسا',\n",
       " 'أخبر',\n",
       " 'أرى',\n",
       " 'أعلم',\n",
       " 'أنبأ',\n",
       " 'حدَث',\n",
       " 'خبَّر',\n",
       " 'نبَّا',\n",
       " 'أفعل به',\n",
       " 'ما أفعله',\n",
       " 'بئس',\n",
       " 'ساء',\n",
       " 'طالما',\n",
       " 'قلما',\n",
       " 'لات',\n",
       " 'لكنَّ',\n",
       " 'ءَ',\n",
       " 'أجل',\n",
       " 'إذاً',\n",
       " 'أمّا',\n",
       " 'إمّا',\n",
       " 'إنَّ',\n",
       " 'أنًّ',\n",
       " 'أى',\n",
       " 'إى',\n",
       " 'أيا',\n",
       " 'ب',\n",
       " 'ثمَّ',\n",
       " 'جلل',\n",
       " 'جير',\n",
       " 'رُبَّ',\n",
       " 'س',\n",
       " 'علًّ',\n",
       " 'ف',\n",
       " 'كأنّ',\n",
       " 'كلَّا',\n",
       " 'كى',\n",
       " 'ل',\n",
       " 'لات',\n",
       " 'لعلَّ',\n",
       " 'لكنَّ',\n",
       " 'لكنَّ',\n",
       " 'م',\n",
       " 'نَّ',\n",
       " 'هلّا',\n",
       " 'وا',\n",
       " 'أل',\n",
       " 'إلّا',\n",
       " 'ت',\n",
       " 'ك',\n",
       " 'لمّا',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ا',\n",
       " 'ي',\n",
       " 'تجاه',\n",
       " 'تلقاء',\n",
       " 'جميع',\n",
       " 'حسب',\n",
       " 'سبحان',\n",
       " 'شبه',\n",
       " 'لعمر',\n",
       " 'مثل',\n",
       " 'معاذ',\n",
       " 'أبو',\n",
       " 'أخو',\n",
       " 'حمو',\n",
       " 'فو',\n",
       " 'مئة',\n",
       " 'مئتان',\n",
       " 'ثلاثمئة',\n",
       " 'أربعمئة',\n",
       " 'خمسمئة',\n",
       " 'ستمئة',\n",
       " 'سبعمئة',\n",
       " 'ثمنمئة',\n",
       " 'تسعمئة',\n",
       " 'مائة',\n",
       " 'ثلاثمائة',\n",
       " 'أربعمائة',\n",
       " 'خمسمائة',\n",
       " 'ستمائة',\n",
       " 'سبعمائة',\n",
       " 'ثمانمئة',\n",
       " 'تسعمائة',\n",
       " 'عشرون',\n",
       " 'ثلاثون',\n",
       " 'اربعون',\n",
       " 'خمسون',\n",
       " 'ستون',\n",
       " 'سبعون',\n",
       " 'ثمانون',\n",
       " 'تسعون',\n",
       " 'عشرين',\n",
       " 'ثلاثين',\n",
       " 'اربعين',\n",
       " 'خمسين',\n",
       " 'ستين',\n",
       " 'سبعين',\n",
       " 'ثمانين',\n",
       " 'تسعين',\n",
       " 'بضع',\n",
       " 'نيف',\n",
       " 'أجمع',\n",
       " 'جميع',\n",
       " 'عامة',\n",
       " 'عين',\n",
       " 'نفس',\n",
       " 'لا سيما',\n",
       " 'أصلا',\n",
       " 'أهلا',\n",
       " 'أيضا',\n",
       " 'بؤسا',\n",
       " 'بعدا',\n",
       " 'بغتة',\n",
       " 'تعسا',\n",
       " 'حقا',\n",
       " 'حمدا',\n",
       " 'خلافا',\n",
       " 'خاصة',\n",
       " 'دواليك',\n",
       " 'سحقا',\n",
       " 'سرا',\n",
       " 'سمعا',\n",
       " 'صبرا',\n",
       " 'صدقا',\n",
       " 'صراحة',\n",
       " 'طرا',\n",
       " 'عجبا',\n",
       " 'عيانا',\n",
       " 'غالبا',\n",
       " 'فرادى',\n",
       " 'فضلا',\n",
       " 'قاطبة',\n",
       " 'كثيرا',\n",
       " 'لبيك',\n",
       " 'معاذ',\n",
       " 'أبدا',\n",
       " 'إزاء',\n",
       " 'أصلا',\n",
       " 'الآن',\n",
       " 'أمد',\n",
       " 'أمس',\n",
       " 'آنفا',\n",
       " 'آناء',\n",
       " 'أنّى',\n",
       " 'أول',\n",
       " 'أيّان',\n",
       " 'تارة',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'حقا',\n",
       " 'صباح',\n",
       " 'مساء',\n",
       " 'ضحوة',\n",
       " 'عوض',\n",
       " 'غدا',\n",
       " 'غداة',\n",
       " 'قطّ',\n",
       " 'كلّما',\n",
       " 'لدن',\n",
       " 'لمّا',\n",
       " 'مرّة',\n",
       " 'قبل',\n",
       " 'خلف',\n",
       " 'أمام',\n",
       " 'فوق',\n",
       " 'تحت',\n",
       " 'يمين',\n",
       " 'شمال',\n",
       " 'ارتدّ',\n",
       " 'استحال',\n",
       " 'أصبح',\n",
       " 'أضحى',\n",
       " 'آض',\n",
       " 'أمسى',\n",
       " 'انقلب',\n",
       " 'بات',\n",
       " 'تبدّل',\n",
       " 'تحوّل',\n",
       " 'حار',\n",
       " 'رجع',\n",
       " 'راح',\n",
       " 'صار',\n",
       " 'ظلّ',\n",
       " 'عاد',\n",
       " 'غدا',\n",
       " 'كان',\n",
       " 'ما انفك',\n",
       " 'ما برح',\n",
       " 'مادام',\n",
       " 'مازال',\n",
       " 'مافتئ',\n",
       " 'ابتدأ',\n",
       " 'أخذ',\n",
       " 'اخلولق',\n",
       " 'أقبل',\n",
       " 'انبرى',\n",
       " 'أنشأ',\n",
       " 'أوشك',\n",
       " 'جعل',\n",
       " 'حرى',\n",
       " 'شرع',\n",
       " 'طفق',\n",
       " 'علق',\n",
       " 'قام',\n",
       " 'كرب',\n",
       " 'كاد',\n",
       " 'هبّ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_ar_stopwords(text):\n",
    "    \"\"\"\n",
    "        Here we remove all Arabic stop words\n",
    "   \"\"\"     \n",
    "    original_words = []\n",
    "    # Tokenize the sentence to tokens\n",
    "    words = word_tokenize(text) # it works on one word not list\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            original_words.append(word)\n",
    "    filtered_sentence = \" \".join(original_words)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اللغة العربية اللغات السامية\n"
     ]
    }
   ],
   "source": [
    "print(removing_ar_stopwords(\"اللغة العربية هي أم اللغات السامية\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        string of words\n",
    "    return:\n",
    "        string of words but standardize the words\n",
    "    '''\n",
    "    sentence = re.sub(\"[إأآا]\", \"ا\", sentence)\n",
    "    sentence = re.sub(\"ى\", \"ي\", sentence)\n",
    "    sentence = re.sub(\"ؤ\", \"ء\", sentence)\n",
    "    sentence = re.sub(\"ئ\", \"ء\", sentence)\n",
    "    sentence = re.sub(\"ة\", \"ه\", sentence)\n",
    "    sentence = re.sub(\"گ\", \"ك\", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قال الشاعر حافظ ابراهيم علي لسان اللغه العربيه\n"
     ]
    }
   ],
   "source": [
    "print(normalize(\"قال الشاعر حافظ إبراهيم على لسان اللغة العربية\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_ISRIStemmer(text):\n",
    "\n",
    "    st = ISRIStemmer()\n",
    "    stemmend_words = []\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        stemmend_words.append(st.stem(word))\n",
    "    stemmed_sentence = \" \".join(stemmend_words)\n",
    "    return stemmed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قال شعر حفظ راهيم على لسن لغة عرب\n"
     ]
    }
   ],
   "source": [
    "print(stemming_ISRIStemmer(\"قال الشاعر حافظ إبراهيم على لسان اللغة العربية\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camel Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A suite of Arabic natural language processing tools developed by the CAMeL Lab at New York University Abu Dhabi.\n",
    "\n",
    "- Documentation link: https://camel-tools.readthedocs.io/en/latest/\n",
    "- Github link: https://github.com/CAMeL-Lab/camel_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install camel-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>وَسِعْتُ كِتابَ اللَّهِ لفظـــاً وَحِكمَــــةً</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>وَما ضِقْتُ عن آيٍ به وَعِظــــاتِ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>فَكيفَ أَضيقُ اليومَ عن وَصْفِ آلــةٍ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>وَتَنسيـــقُ أسمــاءٍ لِمُخْتَرَعــــاتِ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Sentences\n",
       "0  وَسِعْتُ كِتابَ اللَّهِ لفظـــاً وَحِكمَــــةً\n",
       "1              وَما ضِقْتُ عن آيٍ به وَعِظــــاتِ\n",
       "2           فَكيفَ أَضيقُ اليومَ عن وَصْفِ آلــةٍ\n",
       "3        وَتَنسيـــقُ أسمــاءٍ لِمُخْتَرَعــــاتِ"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Dataframe of Arabic texts\n",
    "import pandas as pd\n",
    "  \n",
    "data = [\"وَسِعْتُ كِتابَ اللَّهِ لفظـــاً وَحِكمَــــةً\", \"وَما ضِقْتُ عن آيٍ به وَعِظــــاتِ\", \"فَكيفَ أَضيقُ اليومَ عن وَصْفِ آلــةٍ\", \"وَتَنسيـــقُ أسمــاءٍ لِمُخْتَرَعــــاتِ\"]\n",
    "df = pd.DataFrame(data, columns=['Sentences'])\n",
    "  \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>وسعت كتاب الله لفظا وحكمة</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>وما ضقت عن آي به وعظات</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>فكيف أضيق اليوم عن وصف آلة</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>وتنسيق أسماء لمخترعات</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Sentences\n",
       "0   وسعت كتاب الله لفظا وحكمة\n",
       "1      وما ضقت عن آي به وعظات\n",
       "2  فكيف أضيق اليوم عن وصف آلة\n",
       "3       وتنسيق أسماء لمخترعات"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dediacritization tool\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "# apply to your text column\n",
    "df.Sentences = df.Sentences.apply(dediac_ar)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>وسعت كتاب الله لفظا وحكمه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>وما ضقت عن اي به وعظات</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>فكيف اضيق اليوم عن وصف اله</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>وتنسيق اسماء لمخترعات</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Sentences\n",
       "0   وسعت كتاب الله لفظا وحكمه\n",
       "1      وما ضقت عن اي به وعظات\n",
       "2  فكيف اضيق اليوم عن وصف اله\n",
       "3       وتنسيق اسماء لمخترعات"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_ar\n",
    "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
    "\n",
    "# Normalization on sentence by removing the dots from the teh-marbuta and the hamza from the alef\n",
    "\n",
    "def normalize(text):\n",
    "    text = normalize_alef_maksura_ar(text)\n",
    "    text = normalize_alef_ar(text)\n",
    "    text = normalize_teh_marbuta_ar(text)\n",
    "    return text\n",
    "  \n",
    "df.Sentences = df.Sentences.apply(normalize)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[وسعت, كتاب, الله, لفظا, وحكمه]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[وما, ضقت, عن, اي, به, وعظات]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[فكيف, اضيق, اليوم, عن, وصف, اله]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[وتنسيق, اسماء, لمخترعات]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Sentences\n",
       "0    [وسعت, كتاب, الله, لفظا, وحكمه]\n",
       "1      [وما, ضقت, عن, اي, به, وعظات]\n",
       "2  [فكيف, اضيق, اليوم, عن, وصف, اله]\n",
       "3          [وتنسيق, اسماء, لمخترعات]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization of sentences\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "\n",
    "df.Sentences = df.Sentences.apply(simple_word_tokenize)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some other good tools for Arabic preprocessing like **Qalsadi** which is an Arabic mophological analyzer Library for python. it has many features like:\n",
    "- Lemmatization\n",
    "- Vocalized Text Analyzer,\n",
    "- give word frequency in Arabic modern use.\n",
    "\n",
    "Github link: https://github.com/linuxscout/qalsadi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are a number of other good options out there (like Farasa, MADAMIRA and Stanford CoreNLP) camel-tools found to be the most versatile, comprehensive, and easy to use of them. but you could use any library depending on the task in hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Features Extraction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One- hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is a technique used to convert categorical variables into a format that can be easily understood by machine learning algorithms. It creates a binary vector for each category in the variable, where all values are zero except for the category being represented, which is set to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  team  points\n",
      "0    A      25\n",
      "1    A      12\n",
      "2    B      15\n",
      "3    B      14\n",
      "4    B      19\n",
      "5    B      23\n",
      "6    C      25\n",
      "7    C      29\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'],\n",
    "                   'points': [25, 12, 15, 14, 19, 23, 25, 29]})\n",
    "\n",
    "#view DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  team  points    0    1    2\n",
      "0    A      25  1.0  0.0  0.0\n",
      "1    A      12  1.0  0.0  0.0\n",
      "2    B      15  0.0  1.0  0.0\n",
      "3    B      14  0.0  1.0  0.0\n",
      "4    B      19  0.0  1.0  0.0\n",
      "5    B      23  0.0  1.0  0.0\n",
      "6    C      25  0.0  0.0  1.0\n",
      "7    C      29  0.0  0.0  1.0\n"
     ]
    }
   ],
   "source": [
    "#creating instance of one-hot-encoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#perform one-hot encoding on 'team' column \n",
    "encoder_df = pd.DataFrame(encoder.fit_transform(df[['team']]).toarray())\n",
    "\n",
    "#merge one-hot encoded columns back with original DataFrame\n",
    "final_df = df.join(encoder_df)\n",
    "\n",
    "#view final df\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bag-of-words model is a technique that represents text as a \"bag\" of individual words, disregarding grammar and word order but keeping track of the frequency of each word. In this model, a document is represented as a vector of word frequencies, where each dimension corresponds to a specific word. BoW is a simple and effective approach to text representation, but it ignores the context and order of words in a sentence or document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Collect Data** \n",
    "\n",
    "وَسِعْتُ كِتابَ اللَّهِ لفظـــاً وَحِكمَــــةً\n",
    "\n",
    "وَما ضِقْتُ عن آيٍ به وَعِظــــاتِ\n",
    "\n",
    "فَكيفَ أَضيقُ اليومَ عن وَصْفِ آلــةٍ\n",
    "\n",
    "وَتَنسيـــقُ أسمــاءٍ لِمُخْتَرَعــــاتِ\n",
    "\n",
    "For this small example, let’s treat each line as a separate “document” and the 4 lines as our entire corpus of documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Design the Vocabulary**\n",
    "\n",
    "Now we can make a list of all of the words in our model vocabulary.\n",
    "\n",
    "The unique words here (the data is not preprocessed) are:\n",
    "\n",
    "“وَسِعْتُ”\n",
    "“كِتابَ”\n",
    "“اللَّهِ”\n",
    "“لفظـــاً”\n",
    "“وَحِكمَــــةً”\n",
    "“وَما”\n",
    "“ضِقْتُ”\n",
    "“عن”\n",
    "“آيٍ”\n",
    "“به”\n",
    "“وَعِظــــاتِ”\n",
    "“فَكيفَ”\n",
    "“أَضيقُ”\n",
    "“اليومَ”\n",
    "“وَصْفِ”\n",
    "“آلــةٍ”\n",
    "“وَتَنسيـــقُ”\n",
    "“أسمــاءٍ”\n",
    "“لِمُخْتَرَعــــاتِ”\n",
    "\n",
    "That is a vocabulary of 19 words from a corpus containing 20 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Create Document Vectors**\n",
    "\n",
    "The next step is to score the words in each document.\n",
    "\n",
    "The objective is to turn each document of free text into a vector that we can use as input or output for a machine learning model.\n",
    "\n",
    "Because we know the vocabulary has 19 words, we can use a fixed-length document representation of 19, with one position in the vector to score each word.\n",
    "\n",
    "The simplest scoring method is to mark the presence of words as a boolean value, 0 for absent, 1 for present.\n",
    "\n",
    "Using the arbitrary ordering of words listed above in our vocabulary, we can step through the first document (“وَسِعْتُ كِتابَ اللَّهِ لفظـــاً وَحِكمَــــةً“) and convert it into a binary vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 : “وَسِعْتُ”  \n",
    "1 : “كِتابَ”\n",
    "1 : “اللَّهِ”\n",
    "1 : “لفظـــاً”\n",
    "1 : “وَحِكمَــــةً”\n",
    "0 : “وَما”\n",
    "0 : “ضِقْتُ”\n",
    "0 : “عن”\n",
    "0 : “آيٍ”\n",
    "0 : “به”\n",
    "0 : “وَعِظــــاتِ”\n",
    "0 : “فَكيفَ”\n",
    "0 : “أَضيقُ”\n",
    "0 : “اليومَ”\n",
    "0 : “وَصْفِ”\n",
    "0 : “آلــةٍ”\n",
    "0 : “وَتَنسيـــقُ”\n",
    "0 : “أسمــاءٍ”\n",
    "0 : “لِمُخْتَرَعــــاتِ”\n",
    "\n",
    "D1 = [1, 1, 1, 1, 1 , 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Salma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_bag_of_words(sentences):\n",
    "    # Tokenize the sentences into words\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # Count the occurrence of each word\n",
    "    word_counts = Counter()\n",
    "    for sentence in tokenized_sentences:\n",
    "        word_counts.update(sentence)\n",
    "\n",
    "    # Create a dictionary with each word as the key and its count as the value\n",
    "    bag_of_words = dict(word_counts)\n",
    "\n",
    "    # Create a list of all unique words in the bag of words\n",
    "    unique_words = list(bag_of_words.keys())\n",
    "\n",
    "    # Create a binary representation for each sentence\n",
    "    binary_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        binary_sentence = []\n",
    "        for word in unique_words:\n",
    "            if word in sentence:\n",
    "                binary_sentence.append(1)\n",
    "            else:\n",
    "                binary_sentence.append(0)\n",
    "        binary_sentences.append(binary_sentence)\n",
    "\n",
    "    return bag_of_words, binary_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BOW for the corpus:\n",
      "\n",
      "{'وَسِعْتُ': 1, 'كِتابَ': 1, 'اللَّهِ': 1, 'لفظـــاً': 1, 'وَحِكمَــــةً': 1, 'وَما': 1, 'ضِقْتُ': 1, 'عن': 2, 'آيٍ': 1, 'به': 1, 'وَعِظــــاتِ': 1, 'فَكيفَ': 1, 'أَضيقُ': 1, 'اليومَ': 1, 'وَصْفِ': 1, 'آلــةٍ': 1, 'وَتَنسيـــقُ': 1, 'أسمــاءٍ': 1, 'لِمُخْتَرَعــــاتِ': 1}\n",
      "The Encoding for each sentence in the corpus:\n",
      "\n",
      "[[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"وَسِعْتُ كِتابَ اللَّهِ لفظـــاً وَحِكمَــــةً\", \"وَما ضِقْتُ عن آيٍ به وَعِظــــاتِ\", \"فَكيفَ أَضيقُ اليومَ عن وَصْفِ آلــةٍ\", \"وَتَنسيـــقُ أسمــاءٍ لِمُخْتَرَعــــاتِ\"]\n",
    "bow, binary_bow = arabic_bag_of_words(sentences)\n",
    "print(\"The BOW for the corpus:\\n\")\n",
    "print(bow)\n",
    "print(\"The Encoding for each sentence in the corpus:\\n\")\n",
    "print(binary_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that As the vocabulary size increases, so does the vector representation of documents. Which leads to a vector with lots of zero scores, called a sparse vector or sparse representation.\n",
    "\n",
    "Sparse vectors require more memory and computational resources when modeling and the vast number of positions or dimensions can make the modeling process very challenging for traditional algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated approach is to create a vocabulary of grouped words. This both changes the scope of the vocabulary and allows the bag-of-words to capture a little bit more meaning from the document.\n",
    "\n",
    "In this approach, each word or token is called a “gram”. Creating a vocabulary of two-word pairs is, in turn, called a bigram model. Again, only the bigrams that appear in the corpus are modeled, not all possible bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N- grams models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams refer to a sequence of N words or characters. N-grams captures the context in which the words are used together. For example, it might be a good idea to consider bigrams like “New York” instead of breaking it into individual words like “New” and “York”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('وَما', 'ضِقْتُ')\n",
      "('ضِقْتُ', 'عن')\n",
      "('عن', 'آيٍ')\n",
      "('آيٍ', 'به')\n",
      "('به', 'وَعِظــــاتِ')\n"
     ]
    }
   ],
   "source": [
    "# Example on bi-grams model\n",
    "from nltk import ngrams\n",
    " \n",
    "sentence = 'وَما ضِقْتُ عن آيٍ به وَعِظــــاتِ'\n",
    " \n",
    "ngram = ngrams(sentence.split(' '), n=2)\n",
    " \n",
    "for x in ngram:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Parsing in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7eeGMHk0OP1"
   },
   "source": [
    "Parsing in natural language processing (NLP) refers to the process of analyzing the grammatical structure of a sentence to determine its meaning. Specifically, parsing involves breaking down a sentence into its constituent parts (such as nouns, verbs, adjectives, etc.) and analyzing how these parts are related to each other.\n",
    "\n",
    "There are 2 types of parsing:\n",
    "1. Dependency parsing\n",
    "2. constituency parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXgCtNBY6y1k"
   },
   "source": [
    "# 1 - Dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency parsing involves identifying the syntactic dependencies between words in a sentence, i.e., which words modify or depend on other words in the sentence. It produces a tree structure that shows the relationships between words and their dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you could download the stanford parser from here: https://nlp.stanford.edu/software/lex-parser.shtml#Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing on English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NCHfUBxmyNBT"
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "BwdN1nvLyRzR"
   },
   "outputs": [],
   "source": [
    "# Define the path to the Stanford Parser\n",
    "stanford_parser_path = \"C:/Users/Salma/Downloads/NLP/stanford-parser-full-2020-11-17/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "xLDu8vMnyZmU"
   },
   "outputs": [],
   "source": [
    "# Define a sample sentence\n",
    "sentence = \"The cat sat on the mat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "AxV3IaIY04LE",
    "outputId": "c9c6bf49-4c90-4243-95ea-a077d9163583"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Load the Stanford Parser\n",
    "dep_parser = StanfordDependencyParser(\n",
    "    path_to_jar=stanford_parser_path + \"stanford-parser.jar\",\n",
    "    path_to_models_jar=stanford_parser_path + \"stanford-parser-4.2.0-models.jar\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('sat', 'VBD'), 'nsubj', ('cat', 'NN'))\n",
      "(('cat', 'NN'), 'det', ('The', 'DT'))\n",
      "(('sat', 'VBD'), 'obl', ('mat', 'NN'))\n",
      "(('mat', 'NN'), 'case', ('on', 'IN'))\n",
      "(('mat', 'NN'), 'det', ('a', 'DT'))\n"
     ]
    }
   ],
   "source": [
    "# Parse the sentence and extract the dependencies\n",
    "parse = dep_parser.raw_parse(sentence)\n",
    "dep = parse.__next__()\n",
    "for triple in dep.triples():\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma\\Anaconda3\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:579: MatplotlibDeprecationWarning: \n",
      "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
      "  if not cb.iterable(width):\n",
      "C:\\Users\\Salma\\Anaconda3\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:676: MatplotlibDeprecationWarning: \n",
      "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
      "  if cb.iterable(node_size):  # many node sizes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing networkx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# Using reverse() to reverse the direction of edges as nx_graph() returns inverted edges\n",
    "G = dep.nx_graph().reverse()\n",
    "# nx_graph() returns numeric node labels starting from 1\n",
    "# Create a dictionary to map numeric nodes and words in the sentence\n",
    "words = sentence.split(\" \")\n",
    "labels = {index + 1: words[index] for index in range(len(words))}\n",
    "nx.draw(G, with_labels=True, labels=labels, node_size=2500, node_color='#B5EAD7', font_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing on Arabic Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Arabic text we will use Official Stanford NLP Python Library for Many Human Languages (**stanza**): https://github.com/stanfordnlp/stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is a Python natural language processing (NLP) library that provides a suite of accurate and efficient tools for a range of NLP tasks such as tokenization, part-of-speech (POS) tagging, named entity recognition (NER), dependency parsing, and sentiment analysis. It is built on top of Stanford's CoreNLP library and is designed to be easy to use and accessible to non-experts in NLP while also providing state-of-the-art performance. Stanza supports over 70 languages, including Arabic, English, Chinese, and Spanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to Stanza documentation: https://stanfordnlp.github.io/stanza/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and you could use this repo as an inference to stanford API and NLTK \n",
    "to parse Arabic text: https://github.com/ahmednabil950/Arabic_Parser_NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could find the NLTK Corenlp documentation here: https://www.nltk.org/_modules/nltk/parse/corenlp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b55222f9ac4fe29006f1e0b0e40b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 23:25:18 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2023-03-13 23:25:25 INFO: File exists: C:\\Users\\Salma\\AppData\\Roaming\\SPB_16.6\\stanza_resources\\ar\\default.zip\n",
      "2023-03-13 23:25:41 INFO: Finished downloading models and saved to C:\\Users\\Salma\\AppData\\Roaming\\SPB_16.6\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "# Download the Arabic model\n",
    "stanza.download('ar')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 23:26:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d859a94c8e4354a9dd41de4c5131dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 23:27:01 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "| pos       | padt    |\n",
      "| lemma     | padt    |\n",
      "| depparse  | padt    |\n",
      "| ner       | aqmar   |\n",
      "=======================\n",
      "\n",
      "2023-03-13 23:27:01 INFO: Use device: cpu\n",
      "2023-03-13 23:27:01 INFO: Loading: tokenize\n",
      "2023-03-13 23:27:01 INFO: Loading: mwt\n",
      "2023-03-13 23:27:01 INFO: Loading: pos\n",
      "2023-03-13 23:27:02 INFO: Loading: lemma\n",
      "2023-03-13 23:27:02 INFO: Loading: depparse\n",
      "2023-03-13 23:27:06 INFO: Loading: ner\n",
      "2023-03-13 23:27:10 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('ar') # This sets up a default neural pipeline in Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"ضحكة خلود جميلة\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"ضحكة\",\n",
       "      \"lemma\": \"ضَحكَة\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"N------S1I\",\n",
       "      \"feats\": \"Case=Nom|Definite=Cons|Number=Sing\",\n",
       "      \"head\": 0,\n",
       "      \"deprel\": \"root\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 4,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 2,\n",
       "      \"text\": \"خلود\",\n",
       "      \"lemma\": \"خُلُود\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"N------S1I\",\n",
       "      \"feats\": \"Case=Nom|Definite=Ind|Number=Sing\",\n",
       "      \"head\": 1,\n",
       "      \"deprel\": \"nmod\",\n",
       "      \"start_char\": 5,\n",
       "      \"end_char\": 9,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 3,\n",
       "      \"text\": \"جميلة\",\n",
       "      \"lemma\": \"جَمِيل\",\n",
       "      \"upos\": \"ADJ\",\n",
       "      \"xpos\": \"A-----FS1I\",\n",
       "      \"feats\": \"Case=Nom|Definite=Ind|Gender=Fem|Number=Sing\",\n",
       "      \"head\": 1,\n",
       "      \"deprel\": \"amod\",\n",
       "      \"start_char\": 10,\n",
       "      \"end_char\": 15,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"text\": the actual word in the text.\n",
    "- \"lemma\": the base form of the word.\n",
    "- \"upos\": the universal part-of-speech tag for the word.\n",
    "- \"xpos\": the language-specific part-of-speech tag for the word.\n",
    "- \"feats\": morphological features of the word, such as tense, mood, and gender.\n",
    "- \"head\": the index of the word's syntactic head (in this case, the root of the sentence).\n",
    "- \"deprel\": the dependency relation between this word and its syntactic head (in this case, \"root\").\n",
    "- \"start_char\" and \"end_char\": the character offsets of the word in the input text.\n",
    "- \"ner\": the named entity tag for the word (if any).\n",
    "- \"multi_ner\": the named entity tags assigned to the word by multiple named entity recognizers (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ضحكة', 0, 'root')\n",
      "('خلود', 1, 'nmod')\n",
      "('جميلة', 1, 'amod')\n"
     ]
    }
   ],
   "source": [
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tuple represents the following: (word/token, index of head of the dependency, type of the dependency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can access a range of annotations and linguistic information using different attributes and methods provided by the pipeline (stanza.Pipeline()) object. Here are some examples:\n",
    "\n",
    "- doc.sentences: this returns a list of Sentence objects, where each sentence contains a list of Token objects representing the individual words in the sentence.\n",
    "- doc.sentences[i].words[j].text: this returns the text of the jth word in the ith sentence.\n",
    "- doc.sentences[i].words[j].lemma: this returns the lemmatized form of the jth word in the ith sentence.\n",
    "- doc.sentences[i].words[j].upos: this returns the universal part-of-speech tag of the jth word in the ith sentence.\n",
    "- doc.sentences[i].words[j].xpos: this returns the treebank-specific part-of-speech tag of the jth word in the ith sentence.\n",
    "- doc.sentences[i].words[j].feats: this returns a dictionary of morphological features for the jth word in the ith sentence.\n",
    "- doc.sentences[i].words[j].head: this returns the index of the syntactic head of the jth word in the ith sentence.\n",
    "- doc.sentences[i].words[j].deprel: this returns the dependency relation between the jth word in the ith sentence and its syntactic head.\n",
    "- doc.entities: this returns a list of Entity objects representing named entities in the document.\n",
    "- doc.entities[i].text: this returns the text of the ith named entity.\n",
    "- doc.entities[i].type: this returns the type of the ith named entity (e.g., \"PERSON\", \"ORG\", \"LOC\", etc.).\n",
    "- doc.sentences[i].print_tokens(): this prints out the tokens (words) in the ith sentence.\n",
    "- doc.sentences[i].print_dependencies(): this prints out the dependency relations between words in the ith sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lSogIN366bt"
   },
   "source": [
    "# 2- Constituency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constituency parsing involves identifying the syntactic constituents in a sentence, i.e., the parts of the sentence that can be grouped together based on their grammatical role. It produces a tree structure that shows the hierarchical structure of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constituency refers to the hierarchial structure of a sentence where words are grouped together into phrases, and phrases are grouped together into larger constituents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Syntactic Tree, nodes represent the constituent, and the edges represent the syntactic relationships between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTEgXz0oeVla"
   },
   "source": [
    "For various kinds of grammars and parsing methods, NLTK offers a number of parser functions. The following are a few of the primary processor functions:\n",
    "\n",
    "\n",
    "*   **nltk.parse.RecursiveDescentParser**: This top-down parser breaks down phrases based on a context-free grammar using recursive descent parsing.\n",
    "\n",
    "\n",
    "*   **nltk.parse.EarleyChartParser**: Based on a context-free grammar, this bottom-up parser parses words using dynamic programming.\n",
    "*   **nltk.parse.ShiftReduceParser**: This bottom-up parser breaks down words based on a context-free grammar using shift-reduce parsing.\n",
    "\n",
    "\n",
    "*   **nltk.parse.ChartParser**: Based on a context-free language, this top-down parser parses sentences using dynamic programming.\n",
    "\n",
    "\n",
    "*   **nltk.parse.BottomUpChartParser**: Based on a context-free grammar, this bottom-up parser parses words using dynamic programming.\n",
    "\n",
    "\n",
    "\n",
    "Various grammar classes, including context-free grammars, dependency grammars, and probabilistic context-free grammars, can be used with these parser functions. Additionally, they can be used with a variety of input formats, including plain text, tokenized text, and text with part-of-speech tags.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing on English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SjmwKPjc7Iep",
    "outputId": "b3605cc7-e94e-484f-b8e2-913860abec41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Salma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Salma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "HTuBz_v27Mct"
   },
   "outputs": [],
   "source": [
    "# Define a sentence to parse\n",
    "sentence = \"I shot an elephant in my pajamas\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "uRXUeZcu7Q_4"
   },
   "outputs": [],
   "source": [
    "# Tokenize the sentence into words\n",
    "words = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8nxhC1_7gV2",
    "outputId": "a9250cf3-1b38-4078-c8cc-9f480cebe78c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('shot', 'VBP'),\n",
       " ('an', 'DT'),\n",
       " ('elephant', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('pajamas', 'NN')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part of speech tag for each word in the sentence\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "EmMJhP2C70TB"
   },
   "outputs": [],
   "source": [
    "# Define a grammar and create a parser\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N | 'I'\n",
    "    VP -> V NP | VP PP\n",
    "    PP -> P NP\n",
    "    Det -> 'an' | 'my'\n",
    "    N -> 'elephant' | 'pajamas'\n",
    "    V -> 'shot'\n",
    "    P -> 'in'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "rX85ma2F75Ef"
   },
   "outputs": [],
   "source": [
    "# Create a parser based on the grammar\n",
    "parser = nltk.ChartParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "RsJAMc7v7-bG"
   },
   "outputs": [],
   "source": [
    "# Parse the sentence using the parser\n",
    "trees = parser.parse(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5XEAGYi0eL8K",
    "outputId": "9c89550c-2184-4c0e-c724-497718913cce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n"
     ]
    }
   ],
   "source": [
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you could `stanza` to do the Constituency parsing: https://stanfordnlp.github.io/stanza/constituency.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dependency parsing** is typically used for tasks that require analyzing the relationships between words in a sentence, such as part-of-speech tagging, named entity recognition, and machine translation. Dependency parsing is also useful for tasks that require understanding the syntactic structure of a sentence, such as grammar checking and semantic role labeling.\n",
    "\n",
    "- **Constituency parsing** is useful for tasks that require identifying the constituents of a sentence, such as noun phrases, verb phrases, and prepositional phrases. Constituency parsing is also used in tasks that require understanding the meaning of a sentence, such as sentiment analysis and natural language inference.\n",
    "\n",
    "-  **POS** tagging is concerned with assigning a syntactic category to each word, **NER** is concerned with identifying and classifying named entities, and **parsing** is concerned with analyzing the syntactic structure of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/arabic-nlp-unique-challenges-and-their-solutions-d99e8a87893d\n",
    "- https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing\n",
    "- https://www.analyticsvidhya.com/blog/2021/09/essential-text-pre-processing-techniques-for-nlp/\n",
    "- https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n",
    "- https://aiaspirant.com/bag-of-words/\n",
    "- https://towardsdatascience.com/basics-of-countvectorizer-e26677900f9c#:~:text=Countvectorizer%20is%20a%20method%20to,sparse%20matrix%20as%20shown%20below.\n",
    "- https://www.analyticsvidhya.com/blog/2022/05/a-complete-guide-on-feature-extraction-techniques/\n",
    "- https://medium.com/@niitwork0921/what-is-parsing-in-nlp-69f41b3e5620\n",
    "- https://towardsdatascience.com/natural-language-processing-dependency-parsing-cf094bbbe3f7\n",
    "- https://lucaskohorst.medium.com/constituency-vs-dependency-parsing-8601986e5a52#:~:text=Dependency%20parsing%20displays%20only%20relationships,easier%20to%20read%20and%20understand.\n",
    "- https://www.baeldung.com/cs/constituency-vs-dependency-parsing\n",
    "- https://corenlp.run/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get any Arabic Dataset (of your choice) and apply the preprocessing techniques (mentioned here or not) on it, and do a comparison study on the best libraries or models on behave of performance, easy of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>The deadline for submitting the task is 28/3/2023 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Email: skhaled@nu.edu.eg"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
