{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda71bfb",
   "metadata": {},
   "source": [
    "# Outlines\n",
    "- Seq2seq Models (Case Study 06)\n",
    "- Transformers: BERT (Case Study 07)\n",
    "- Question Answering Vs. Chatbots\n",
    "- Conclusion\n",
    "- References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc814d15",
   "metadata": {},
   "source": [
    "# Seq2seq Model\n",
    "\n",
    "The Sequence to Sequence model (seq2seq) consists of two RNNs — an encoder and a decoder. The encoder reads the input sequence, word by word and emits a context (a function of final hidden state of encoder), which would ideally capture the essence (semantic summary) of the input sequence. Based on this context, the decoder generates the output sequence, one word at a time while looking at the context and the previous word during each timestep.\n",
    "\n",
    "![seq2seq Architecture](1_CkeGXClZ5Xs0MhBc7xFqSA.png)\n",
    "\n",
    "The main objective of a Seq2Seq model is to deal with problems where the input data and the output data encoded as vectors have different dimensions from each other. This is a limitation for the normal Deep Neural Networks that use to work with vectors of same dimension.\n",
    "\n",
    "Check Case Study 06 (Chatbot): https://colab.research.google.com/drive/1oAoVK3hOQ5n-A6CoKG1Bt2eZ6mG78nYE?usp=sharing\n",
    "\n",
    "\n",
    "\n",
    "* Advantages of Seq2Seq Model:\n",
    "\n",
    "     - Variable Input and Output Length\n",
    "     - Contextual Understanding\n",
    "\n",
    "* Disadvantages of Seq2Seq Model:\n",
    "\n",
    "     - Difficulty in Handling Long Sequences\n",
    "     - Over-Reliance on Encoder\n",
    "     - Lack of Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ec974",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT’s goal is to generate a language model, only the encoder mechanism is necessary.\n",
    "\n",
    "\n",
    "![BERT Architecture](BERT-base-architecture.png)\n",
    "\n",
    "\n",
    "- BERT can be used on a wide variety of language tasks:\n",
    "\n",
    "  - Sentiment Analysis\n",
    "  - Question answering\n",
    "  - Text Classification: Arabic Dialect (AraBERT and MarBERT)\n",
    "  - Text generation\n",
    "  - Summarization\n",
    "  \n",
    "- How Does BERT Work:\n",
    "\n",
    "  - Large amount of training data\n",
    "  - Masked Language Model (MLM): 15% tokens are hidden during the training\n",
    "  - Next Sentence Prediction (NSP)\n",
    "  - Transformers (Attention)\n",
    "\n",
    "\n",
    "\n",
    "Check Case Study 07 (QA System): https://colab.research.google.com/drive/1FYBfGBX6TQQO2wBvXAev7n0RTVbZ6evP?usp=sharing\n",
    "\n",
    "- Advantages of Transformers over seq2seq:\n",
    "\n",
    "  - Attention Mechanism\n",
    "  - Parallel Computation\n",
    "  - Scalability\n",
    "  - Context-Aware Encoding\n",
    "  - Bidirectional Encoding\n",
    "  - Handling Variable-Length Input/Output\n",
    "  - Transfer Learning and Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5a0e2d",
   "metadata": {},
   "source": [
    "# Question Answering Vs. Chatbot\n",
    "\n",
    "- Question Answering (QA):\n",
    "\n",
    "QA systems are designed to provide specific answers to user questions based on a given context or knowledge base.\n",
    "\n",
    "The input to a QA system is typically a specific question, and the system's goal is to produce a concise and accurate answer.\n",
    "\n",
    "QA systems often rely on information retrieval and extraction techniques to find relevant information from a structured or unstructured data source.\n",
    "\n",
    "QA systems can be task-oriented, focusing on specific domains or knowledge areas, or they can be open-domain, attempting to answer questions on a wide range of topics.\n",
    "\n",
    "Examples of QA systems include factoid question answering systems (e.g., answering \"Who is the president of the United States?\") and reading comprehension systems (e.g., answering questions about a given passage).\n",
    "\n",
    "- Chatbot:\n",
    "\n",
    "Chatbot systems are designed to simulate human-like conversations and engage in interactive dialogues with users.\n",
    "\n",
    "Chatbots can handle a wide range of user inputs and respond with contextually relevant and meaningful replies.\n",
    "\n",
    "The goal of a chatbot is to provide an interactive conversational experience and assist users with various tasks, provide information, or engage in casual conversations.\n",
    "\n",
    "Chatbots use natural language understanding and generation techniques, and they can be rule-based, retrieval-based, or generative-based depending on the underlying architecture and approach.\n",
    "\n",
    "Chatbots can be designed for specific domains or operate as open-domain conversational agents.\n",
    "\n",
    "Examples of chatbots include customer support chatbots, virtual assistants (e.g., Siri, Alexa), and social chatbots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac1253",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In summary, BERT and Seq2Seq models are widely used in chatbots and QA systems. BERT's strength lies in its ability to understand context and provide accurate responses, while Seq2Seq models excel in generating coherent and relevant text. The use of **prompt engineering** further enhances the performance of these models by allowing developers to guide the output. Overall, these advancements in models and techniques have paved the way for more advanced and customizable chatbots and QA systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a6e47",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://cnvrg.io/seq2seq-model/\n",
    "- https://blog.suriya.app/2016-06-28-easy-seq2seq/\n",
    "- https://vvsmanideep.medium.com/interview-chat-bot-using-seq2seq-model-fe9059fffe64\n",
    "- https://towardsdatascience.com/generative-chatbots-using-the-seq2seq-model-d411c8738ab5\n",
    "- https://arxiv.org/pdf/1706.03762.pdf\n",
    "- https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "- https://arxiv.org/abs/1810.04805\n",
    "- https://machinelearningmastery.com/a-brief-introduction-to-bert/\n",
    "- https://huggingface.co/blog/bert-101#1-what-is-bert-used-for\n",
    "- https://www.pytorchlightning.ai/blog/how-to-fine-tune-bert-with-pytorch-lightning\n",
    "- https://sites.aub.edu.lb/mindlab/2020/02/28/arabert-pre-training-bert-for-arabic-language-understanding/\n",
    "- https://github.com/h9-tect/arabic-lib-transformers\n",
    "- https://huggingface.co/UBC-NLP/MARBERT\n",
    "- https://blog.invgate.com/gpt-3-vs-bert#:~:text=While%20GPT%2D3%20only%20considers,sentence%20or%20phrase%20is%20essential.\n",
    "- https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626\n",
    "- https://www.kaggle.com/datasets/grafstor/simple-dialogs-for-chatbot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
